{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98500c61-bd08-487c-96f2-64c9d70c602b",
   "metadata": {
    "id": "98500c61-bd08-487c-96f2-64c9d70c602b",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Installs and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51be9fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "CUDA device: NVIDIA GeForce RTX 4070 Ti SUPER\n",
      "CUDA version: 11.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Max\\Documents\\Software\\5930\\finenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "# CUDA settings\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "torch.backends.cudnn.enabled = True\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# Check CUDA availability and set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    # Set default CUDA device\n",
    "    torch.cuda.set_device(0)\n",
    "    \n",
    "# Import other libraries\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
    "from datasets import load_dataset, Dataset\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a064f14-44e7-4e92-8add-4405f5c2930d",
   "metadata": {
    "id": "8a064f14-44e7-4e92-8add-4405f5c2930d",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c14999",
   "metadata": {},
   "source": [
    "### load model and tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d182fd36-78a8-4c80-b327-05b00efb5492",
   "metadata": {
    "id": "d182fd36-78a8-4c80-b327-05b00efb5492",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer pad token: <|endoftext|>\n",
      "Tokenizer pad token ID: 50256\n",
      "Attempting to load model directly to device...\n",
      "Model loaded successfully\n",
      "\n",
      "Model device check:\n",
      "Model is on CUDA: True\n"
     ]
    }
   ],
   "source": [
    "model_name = \"distilgpt2\"\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# Set the pad_token to eos_token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Print tokenizer info\n",
    "print(f\"Tokenizer pad token: {tokenizer.pad_token}\")\n",
    "print(f\"Tokenizer pad token ID: {tokenizer.pad_token_id}\")\n",
    "\n",
    "# Create model with explicit config\n",
    "from transformers import AutoConfig\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# Try loading model with device placement in from_pretrained\n",
    "try:\n",
    "    print(\"Attempting to load model directly to device...\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name, \n",
    "        config=config,\n",
    "        device_map=\"auto\" if torch.cuda.is_available() else None,\n",
    "        torch_dtype=torch.float32  # Explicitly set dtype\n",
    "    )\n",
    "    print(\"Model loaded successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model directly to device: {e}\")\n",
    "    print(\"Attempting alternate loading method...\")\n",
    "    # Try alternate loading method\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, config=config)\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"Moving model to CUDA...\")\n",
    "        # Try moving parts of the model gradually\n",
    "        for param in model.parameters():\n",
    "            param.data = param.data.to('cuda')\n",
    "    print(\"Model loading complete\")\n",
    "\n",
    "# Print model device info\n",
    "print(f\"\\nModel device check:\")\n",
    "print(f\"Model is on CUDA: {next(model.parameters()).is_cuda}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1dcbd66",
   "metadata": {},
   "source": [
    "### tokenize function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e323ad15-55f3-45ec-9f42-1be7455a4b94",
   "metadata": {
    "id": "e323ad15-55f3-45ec-9f42-1be7455a4b94",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define max length for the sequences\n",
    "MAX_LENGTH = 512\n",
    "\n",
    "def format_alpaca_prompt(example):\n",
    "    \"\"\"Format the instruction and input into a prompt\"\"\"\n",
    "    if example[\"input\"]:\n",
    "        prompt = f\"### Instruction:\\n{example['instruction']}\\n\\n### Input:\\n{example['input']}\\n\\n### Response:\\n\"\n",
    "    else:\n",
    "        prompt = f\"### Instruction:\\n{example['instruction']}\\n\\n### Response:\\n\"\n",
    "    return prompt\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    \"\"\"Tokenize the texts and prepare them for training\"\"\"\n",
    "    # Tokenize with padding and truncation\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=MAX_LENGTH,\n",
    "        return_tensors=None\n",
    "    )\n",
    "    \n",
    "    labels = tokenized[\"input_ids\"].copy()\n",
    "    \n",
    "    # Find the start of the response for each example\n",
    "    for idx, text in enumerate(examples[\"text\"]):\n",
    "        response_start = text.find(\"### Response:\\n\") + len(\"### Response:\\n\")\n",
    "        # Get prompt tokens\n",
    "        prompt_tokens = len(tokenizer(text[:response_start], \n",
    "                                    truncation=True, \n",
    "                                    max_length=MAX_LENGTH)[\"input_ids\"])\n",
    "        \n",
    "        # Mask out prompt tokens in labels\n",
    "        labels[idx][:prompt_tokens] = [-100] * prompt_tokens\n",
    "        \n",
    "        # Ensure no out-of-bounds indices\n",
    "        if prompt_tokens > MAX_LENGTH:\n",
    "            labels[idx] = [-100] * MAX_LENGTH\n",
    "    \n",
    "    # Convert to PyTorch tensors\n",
    "    input_ids = torch.tensor(tokenized[\"input_ids\"], dtype=torch.long)\n",
    "    attention_mask = torch.tensor(tokenized[\"attention_mask\"], dtype=torch.long)\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "Qnq6iT_cPyEs",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "7f158e5a561048e3b6f590594444c158",
      "f59064cfad2947269049f743394173b1",
      "cff7af7a98dc46c68de733e2790cd9ef",
      "b99a828cd586471e9410e7879f46db81",
      "2aef35d22fb949969639fa94064a4457",
      "62d09efd889d4a8f88ad4b79ba0a361a",
      "46ef3ea2ec0845659b235ddc4c703053",
      "667a2a35cd5b42caa8b17a9d8adadf79",
      "09bda7a799e14df6a0a13abe210e7a69",
      "6f9da49a42f449cf971d8ba60d2ef9a8",
      "11235fb8a02241b1a8893d528f23bda6",
      "d8d2cf0e4f2e493fb72050f21af26b95",
      "19fd950f8a4a4d9caf2c70c926d574fa",
      "f2bb9a78b1d2425bb60ed4d40d4c6a4a",
      "95c413c637784ef29955840ce1406f37",
      "a0067481b71f4dec948e7c4d50faed9b",
      "476ac76184b14bacbb1c6f8afebb44ee",
      "893827e6c2d441b7acece76f5fdaa45e",
      "a345326dcec1442eb416fa74020684ca",
      "deaa7e1969b24b45b2700c2aecdc1c4a",
      "cac2e71a998e47e0a173f5406ae5efb6",
      "9cc805d627e946f895de513ff5e35313"
     ]
    },
    "id": "Qnq6iT_cPyEs",
    "outputId": "64ce133f-0a6f-469e-ed7c-bf336e3b28d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing training dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 46584/46584 [00:21<00:00, 2160.57 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing evaluation dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5176/5176 [00:02<00:00, 1964.88 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Verifying data format:\n",
      "input_ids: shape=torch.Size([512]), dtype=torch.int64\n",
      "attention_mask: shape=torch.Size([512]), dtype=torch.int64\n",
      "labels: shape=torch.Size([512]), dtype=torch.int64\n",
      "\n",
      "Validating training dataset:\n",
      "Found 0 invalid samples\n",
      "\n",
      "Validating evaluation dataset:\n",
      "Found 0 invalid samples\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_alpaca_data(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "def prepare_dataset(data):\n",
    "    \"\"\"Convert the JSON data into a format suitable for the model\"\"\"\n",
    "    formatted_data = []\n",
    "    for item in data:\n",
    "        prompt = format_alpaca_prompt(item)\n",
    "        formatted_data.append({\n",
    "            \"text\": prompt + item[\"output\"]  # Combine prompt and output\n",
    "        })\n",
    "    return Dataset.from_list(formatted_data)\n",
    "\n",
    "# Load and prepare the data\n",
    "alpaca_data = load_alpaca_data('alpaca_data_cleaned.json')\n",
    "train_size = int(0.9 * len(alpaca_data))\n",
    "train_data = alpaca_data[:train_size]\n",
    "eval_data = alpaca_data[train_size:]\n",
    "\n",
    "# Convert to Dataset format\n",
    "train_dataset = prepare_dataset(train_data)\n",
    "eval_dataset = prepare_dataset(eval_data)\n",
    "\n",
    "# Tokenize the datasets with smaller batch size and add error handling\n",
    "def safe_map_tokenization(dataset, batch_size=4):\n",
    "    try:\n",
    "        return dataset.map(\n",
    "            tokenize_function,\n",
    "            batched=True,\n",
    "            batch_size=batch_size,\n",
    "            remove_columns=dataset.column_names\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error during tokenization: {e}\")\n",
    "        raise\n",
    "\n",
    "print(\"Tokenizing training dataset...\")\n",
    "tokenized_train = safe_map_tokenization(train_dataset)\n",
    "print(\"Tokenizing evaluation dataset...\")\n",
    "tokenized_eval = safe_map_tokenization(eval_dataset)\n",
    "\n",
    "# Set the tensor format\n",
    "tokenized_train.set_format(\"torch\")\n",
    "tokenized_eval.set_format(\"torch\")\n",
    "\n",
    "# Verify data format\n",
    "print(\"\\nVerifying data format:\")\n",
    "sample = tokenized_train[0]\n",
    "for key, value in sample.items():\n",
    "    print(f\"{key}: shape={value.shape}, dtype={value.dtype}\")\n",
    "\n",
    "# Add validation check\n",
    "def validate_dataset(dataset, name):\n",
    "    print(f\"\\nValidating {name}:\")\n",
    "    invalid_samples = 0\n",
    "    for i, sample in enumerate(dataset):\n",
    "        if not all(isinstance(v, torch.Tensor) for v in sample.values()):\n",
    "            print(f\"Sample {i} has non-tensor values\")\n",
    "            invalid_samples += 1\n",
    "        if any(v.dtype not in [torch.long, torch.int64] for v in sample.values()):\n",
    "            print(f\"Sample {i} has incorrect dtype\")\n",
    "            invalid_samples += 1\n",
    "    print(f\"Found {invalid_samples} invalid samples\")\n",
    "    return invalid_samples == 0\n",
    "\n",
    "validate_dataset(tokenized_train, \"training dataset\")\n",
    "validate_dataset(tokenized_eval, \"evaluation dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc93b376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sizes - Train: 46584, Eval: 5176\n",
      "Scaled sizes - Train: 2000, Eval: 200\n"
     ]
    }
   ],
   "source": [
    "# Scale datasets for testing\n",
    "def scale_dataset(dataset, max_samples=1000):\n",
    "    \"\"\"Scale down a dataset to a maximum number of samples\"\"\"\n",
    "    if len(dataset) > max_samples:\n",
    "        scaled_indices = list(range(max_samples))\n",
    "        return dataset.select(scaled_indices)\n",
    "    return dataset\n",
    "\n",
    "# Set your desired size\n",
    "MAX_SAMPLES = 2000  # Adjust this number as needed\n",
    "\n",
    "# Scale both datasets\n",
    "print(f\"Original sizes - Train: {len(tokenized_train)}, Eval: {len(tokenized_eval)}\")\n",
    "\n",
    "tokenized_train = scale_dataset(tokenized_train, MAX_SAMPLES)\n",
    "tokenized_eval = scale_dataset(tokenized_eval, max(50, int(MAX_SAMPLES * 0.1)))  # Keep eval set ~10% of train\n",
    "\n",
    "print(f\"Scaled sizes - Train: {len(tokenized_train)}, Eval: {len(tokenized_eval)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bQwXGOaRP1Ks",
   "metadata": {
    "id": "bQwXGOaRP1Ks"
   },
   "source": [
    "# Fine Tuning Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf808eb",
   "metadata": {},
   "source": [
    "### validate setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9eb90268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer vocab size: 50257\n",
      "Model vocab size: 50257\n",
      "input_ids - Min: -100, Max: 50256, Shape: torch.Size([512])\n",
      "attention_mask - Min: 0, Max: 1, Shape: torch.Size([512])\n",
      "labels - Min: -100, Max: 50256, Shape: torch.Size([512])\n"
     ]
    }
   ],
   "source": [
    "# Check vocab sizes and data ranges\n",
    "print(f\"Tokenizer vocab size: {len(tokenizer)}\")\n",
    "print(f\"Model vocab size: {model.config.vocab_size}\")\n",
    "\n",
    "# Function to check tensor values\n",
    "def check_tensor_values(tensor, name):\n",
    "    if isinstance(tensor, torch.Tensor):\n",
    "        print(f\"{name} - Min: {tensor.min().item()}, Max: {tensor.max().item()}, Shape: {tensor.shape}\")\n",
    "\n",
    "# Check a sample from the dataset\n",
    "sample = tokenized_train[0]\n",
    "for key, value in sample.items():\n",
    "    check_tensor_values(value, key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccbe772",
   "metadata": {},
   "source": [
    "### custom collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a74c0186",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Optional, Union, List, Dict, Any\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CustomDataCollatorForLanguageModeling:\n",
    "    tokenizer: AutoTokenizer\n",
    "    mlm: bool = False\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "\n",
    "    def __call__(self, examples: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:\n",
    "        # Extract the relevant fields\n",
    "        input_ids = [example[\"input_ids\"] for example in examples]\n",
    "        attention_mask = [example[\"attention_mask\"] for example in examples]\n",
    "        labels = [example[\"labels\"] for example in examples]\n",
    "\n",
    "        # Convert to tensors if they aren't already\n",
    "        if not isinstance(input_ids[0], torch.Tensor):\n",
    "            input_ids = [torch.tensor(ids, dtype=torch.long) for ids in input_ids]\n",
    "        if not isinstance(attention_mask[0], torch.Tensor):\n",
    "            attention_mask = [torch.tensor(mask, dtype=torch.long) for mask in attention_mask]\n",
    "        if not isinstance(labels[0], torch.Tensor):\n",
    "            labels = [torch.tensor(lab, dtype=torch.long) for lab in labels]\n",
    "\n",
    "        # Pad sequences\n",
    "        max_length = max(ids.size(0) for ids in input_ids)\n",
    "        \n",
    "        def pad_sequence(sequences, pad_value):\n",
    "            result = torch.full((len(sequences), max_length), pad_value, dtype=torch.long)\n",
    "            for i, seq in enumerate(sequences):\n",
    "                length = seq.size(0)\n",
    "                result[i, :length] = seq\n",
    "            return result\n",
    "\n",
    "        # Pad and create batch\n",
    "        input_ids_padded = pad_sequence(input_ids, self.tokenizer.pad_token_id)\n",
    "        attention_mask_padded = pad_sequence(attention_mask, 0)\n",
    "        labels_padded = pad_sequence(labels, -100)\n",
    "\n",
    "        # Ensure values are within vocabulary bounds\n",
    "        vocab_size = len(self.tokenizer)\n",
    "        input_ids_padded = torch.clamp(input_ids_padded, min=0, max=vocab_size-1)\n",
    "        labels_padded = torch.where(\n",
    "            (labels_padded >= 0) & (labels_padded < vocab_size),\n",
    "            labels_padded,\n",
    "            torch.tensor(-100, dtype=torch.long)\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids_padded,\n",
    "            \"attention_mask\": attention_mask_padded,\n",
    "            \"labels\": labels_padded\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723bbb20",
   "metadata": {},
   "source": [
    "### Training args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45d35308-23a8-4628-a218-81f73558f29c",
   "metadata": {
    "id": "45d35308-23a8-4628-a218-81f73558f29c",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test batch shapes:\n",
      "input_ids: torch.Size([2, 512]), dtype: torch.int64, range: [0, 50256]\n",
      "attention_mask: torch.Size([2, 512]), dtype: torch.int64, range: [0, 1]\n",
      "labels: torch.Size([2, 512]), dtype: torch.int64, range: [-100, 50256]\n"
     ]
    }
   ],
   "source": [
    "# Training arguments with safe defaults\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=100,\n",
    "    logging_steps=100,\n",
    "    save_total_limit=2,\n",
    "    logging_dir=\"./logs\",\n",
    "    max_grad_norm=0.5,\n",
    "    gradient_accumulation_steps=16,\n",
    "    fp16=False,\n",
    "    dataloader_pin_memory=False,\n",
    "    remove_unused_columns=False,\n",
    "    prediction_loss_only=True,\n",
    "    seed=42,\n",
    "    full_determinism=False,\n",
    ")\n",
    "\n",
    "# Create custom collator\n",
    "data_collator = CustomDataCollatorForLanguageModeling(tokenizer=tokenizer)\n",
    "\n",
    "# Initialize trainer with custom collator\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_eval,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Test the collator with a small batch\n",
    "test_batch = data_collator([tokenized_train[i] for i in range(2)])\n",
    "print(\"\\nTest batch shapes:\")\n",
    "for k, v in test_batch.items():\n",
    "    print(f\"{k}: {v.shape}, dtype: {v.dtype}, range: [{v.min()}, {v.max()}]\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RcQFXYQuN_Cl",
   "metadata": {
    "id": "RcQFXYQuN_Cl"
   },
   "source": [
    "# Fine Tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e060a29d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 100/375 [00:56<02:30,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.3951, 'grad_norm': 16.91701889038086, 'learning_rate': 1e-05, 'epoch': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 33%|███▎      | 125/375 [01:11<02:16,  1.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7909576296806335, 'eval_runtime': 1.5058, 'eval_samples_per_second': 132.818, 'eval_steps_per_second': 33.205, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 200/375 [01:54<01:36,  1.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7923, 'grad_norm': 10.684296607971191, 'learning_rate': 6.363636363636364e-06, 'epoch': 1.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 67%|██████▋   | 250/375 [02:24<01:08,  1.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7763142585754395, 'eval_runtime': 1.4697, 'eval_samples_per_second': 136.078, 'eval_steps_per_second': 34.019, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 300/375 [02:52<00:42,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7842, 'grad_norm': 18.6118106842041, 'learning_rate': 2.7272727272727272e-06, 'epoch': 2.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      "100%|██████████| 375/375 [03:36<00:00,  1.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7742096185684204, 'eval_runtime': 1.4948, 'eval_samples_per_second': 133.795, 'eval_steps_per_second': 33.449, 'epoch': 3.0}\n",
      "{'train_runtime': 216.2747, 'train_samples_per_second': 27.742, 'train_steps_per_second': 1.734, 'train_loss': 1.4781442565917968, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Clear CUDA cache and start training\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "try:\n",
    "    trainer.train()\n",
    "except RuntimeError as e:\n",
    "    print(f\"Training error: {e}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"\\nCUDA Memory Summary:\")\n",
    "        print(torch.cuda.memory_summary())\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52544118",
   "metadata": {},
   "source": [
    "### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6268221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model and tokenizer...\n",
      "Model and tokenizer saved to ../FinalModels/NEWfine_tuned_alpaca_gpt2\n",
      "\n",
      "Verifying save...\n",
      "✓ Successfully loaded saved model and tokenizer\n"
     ]
    }
   ],
   "source": [
    "# Save paths\n",
    "model_save_path = \"../FinalModels/NEWfine_tuned_alpaca_gpt2\"\n",
    "\n",
    "# Save the model and tokenizer\n",
    "print(\"Saving model and tokenizer...\")\n",
    "trainer.save_model(model_save_path)\n",
    "tokenizer.save_pretrained(model_save_path)\n",
    "print(f\"Model and tokenizer saved to {model_save_path}\")\n",
    "\n",
    "# Quick verification that the save worked\n",
    "print(\"\\nVerifying save...\")\n",
    "try:\n",
    "    # Try to load the model and tokenizer\n",
    "    test_model = AutoModelForCausalLM.from_pretrained(model_save_path)\n",
    "    test_tokenizer = AutoTokenizer.from_pretrained(model_save_path)\n",
    "    print(\"✓ Successfully loaded saved model and tokenizer\")\n",
    "except Exception as e:\n",
    "    print(f\"Error verifying save: {e}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m125"
  },
  "kernelspec": {
   "display_name": "finenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "09bda7a799e14df6a0a13abe210e7a69": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "11235fb8a02241b1a8893d528f23bda6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "19fd950f8a4a4d9caf2c70c926d574fa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_476ac76184b14bacbb1c6f8afebb44ee",
      "placeholder": "​",
      "style": "IPY_MODEL_893827e6c2d441b7acece76f5fdaa45e",
      "value": "Map: 100%"
     }
    },
    "2aef35d22fb949969639fa94064a4457": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "46ef3ea2ec0845659b235ddc4c703053": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "476ac76184b14bacbb1c6f8afebb44ee": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "62d09efd889d4a8f88ad4b79ba0a361a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "667a2a35cd5b42caa8b17a9d8adadf79": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6f9da49a42f449cf971d8ba60d2ef9a8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7f158e5a561048e3b6f590594444c158": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f59064cfad2947269049f743394173b1",
       "IPY_MODEL_cff7af7a98dc46c68de733e2790cd9ef",
       "IPY_MODEL_b99a828cd586471e9410e7879f46db81"
      ],
      "layout": "IPY_MODEL_2aef35d22fb949969639fa94064a4457"
     }
    },
    "893827e6c2d441b7acece76f5fdaa45e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "95c413c637784ef29955840ce1406f37": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cac2e71a998e47e0a173f5406ae5efb6",
      "placeholder": "​",
      "style": "IPY_MODEL_9cc805d627e946f895de513ff5e35313",
      "value": " 188/188 [00:00&lt;00:00, 872.19 examples/s]"
     }
    },
    "9cc805d627e946f895de513ff5e35313": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a0067481b71f4dec948e7c4d50faed9b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a345326dcec1442eb416fa74020684ca": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b99a828cd586471e9410e7879f46db81": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6f9da49a42f449cf971d8ba60d2ef9a8",
      "placeholder": "​",
      "style": "IPY_MODEL_11235fb8a02241b1a8893d528f23bda6",
      "value": " 1836/1836 [00:02&lt;00:00, 898.93 examples/s]"
     }
    },
    "cac2e71a998e47e0a173f5406ae5efb6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cff7af7a98dc46c68de733e2790cd9ef": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_667a2a35cd5b42caa8b17a9d8adadf79",
      "max": 1836,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_09bda7a799e14df6a0a13abe210e7a69",
      "value": 1836
     }
    },
    "d8d2cf0e4f2e493fb72050f21af26b95": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_19fd950f8a4a4d9caf2c70c926d574fa",
       "IPY_MODEL_f2bb9a78b1d2425bb60ed4d40d4c6a4a",
       "IPY_MODEL_95c413c637784ef29955840ce1406f37"
      ],
      "layout": "IPY_MODEL_a0067481b71f4dec948e7c4d50faed9b"
     }
    },
    "deaa7e1969b24b45b2700c2aecdc1c4a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f2bb9a78b1d2425bb60ed4d40d4c6a4a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a345326dcec1442eb416fa74020684ca",
      "max": 188,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_deaa7e1969b24b45b2700c2aecdc1c4a",
      "value": 188
     }
    },
    "f59064cfad2947269049f743394173b1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_62d09efd889d4a8f88ad4b79ba0a361a",
      "placeholder": "​",
      "style": "IPY_MODEL_46ef3ea2ec0845659b235ddc4c703053",
      "value": "Map: 100%"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
