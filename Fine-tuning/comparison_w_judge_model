import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import json
from datetime import datetime
import pandas as pd
from tqdm import tqdm
import openai
import time
from typing import Dict, Any

# Add OpenAI API key configuration
openai.api_key = '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'


# Load models
base_model_id = "HuggingFaceTB/SmolLM2-135M"
instruct_model_path = "5930Final/Fine-tuning/smollm2_finetuned/05"

base_tokenizer = AutoTokenizer.from_pretrained(base_model_id)
instruct_tokenizer = AutoTokenizer.from_pretrained(instruct_model_path, local_files_only=True)

base_model = AutoModelForCausalLM.from_pretrained(base_model_id)
instruct_model = AutoModelForCausalLM.from_pretrained(instruct_model_path, local_files_only=True)

# Evaluation prompts
# Evaluation prompts
prompts = {
    "text_generation": [
        "Write a sentence about a dog.",
        "Complete this sentence: The sky is...",
        "Write a sentence about your favorite food.",
        "Make up a name for a new pet.",
        "Write a short story about a tree.",
        "Create a sentence using the word 'happy'.",
        "Write a thank you message.",
        "Complete: My favorite season is...",
        "Make up a silly superhero name.",
        "Write a sentence about the ocean.",
        "Create a title for a children's book.",
        "Write a short weather report.",
        "Make up a name for a restaurant.",
        "Write a sentence about space.",
        "Create a message for a birthday card.",
        "Write a sentence about a cat.",
        "Complete this sentence: Today I feel...",
        "Write about your dream house.",
        "Make up a name for a new color.",
        "Write a sentence about music.",
        "Create a slogan for ice cream.",
        "Write about your perfect day.",
        "Make up a name for a fictional country.",
        "Write a sentence about rain.",
        "Create a motto for a school."
    ],
    "question_answering": [
        "What color is a banana?",
        "How many legs does a cat have?",
        "What do birds use to fly?",
        "Where do fish live?",
        "What do plants need to grow?",
        "What makes a rainbow appear?",
        "What do people use umbrellas for?",
        "Why do leaves fall from trees?",
        "What do we use to tell time?",
        "How do boats float on water?",
        "What makes ice cream melt?",
        "Why do we need sleep?",
        "What makes the sky look blue?",
        "How do airplanes stay in the air?",
        "What happens to water when it freezes?",
        "Why does the moon shine at night?",
        "What makes a car move forward?",
        "What do cows eat?",
        "How do phones work?",
        "What makes a bike stay up?",
        "Why do we need food?",
        "How do magnets work?",
        "What makes rain fall?",
        "Why do we have seasons?",
        "How do books help us learn?"
    ]
}

def generate_response(model, tokenizer, message, temperature=0.5, max_length=200, system_prompt="", is_instruct=False):
    if is_instruct:
        if system_prompt:
            full_prompt = f"{system_prompt}\n\nHuman: {message}\nAssistant:"
        else:
            full_prompt = f"Human: {message}\nAssistant:"
    else:
        full_prompt = message
    
    inputs = tokenizer(full_prompt, return_tensors="pt")
    
    with torch.no_grad():
        outputs = model.generate(
            inputs.input_ids,
            max_length=max_length,
            do_sample=True,
            temperature=temperature,
            top_k=50,
            top_p=0.95,
            num_return_sequences=1,
            pad_token_id=tokenizer.eos_token_id
        )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    if is_instruct:
        try:
            response = response.split("Assistant:")[-1].strip()
        except:
            pass
    else:
        response = response[len(full_prompt):].strip()
    
    return response

def create_evaluation_prompt(prompt: str, base_response: str, instruct_response: str) -> str:
    return f"""Evaluate these two model responses to the prompt: "{prompt}"

Base model response: "{base_response}"
Instruction-tuned model response: "{instruct_response}"

Score each response on a scale of 0-5 for:
1. Relevance to prompt (how well does it address the given task?)
2. Factual accuracy (are the statements correct?)
3. Coherence/readability (is it well-written and makes sense?)
4. Conciseness (is it appropriately brief without repetition?)
5. Task completion (does it fully complete what was asked?)

Provide scores in JSON format with a brief explanation for each score.
"""



def run_batch_evaluation():
    results = []
    
    # Parameters for generation
    temperature = 0.5
    max_length = 200
    system_prompt = ""
    
    # Process each category
    for category, category_prompts in prompts.items():
        for prompt in tqdm(category_prompts, desc=f"Processing {category}"):
            # Generate responses from both models
            base_response = generate_response(
                base_model,
                base_tokenizer,
                prompt,
                temperature,
                max_length,
                system_prompt,
                is_instruct=False
            )
            
            instruct_response = generate_response(
                instruct_model,
                instruct_tokenizer,
                prompt,
                temperature,
                max_length,
                system_prompt,
                is_instruct=True
            )
            
            # Get GPT-4 evaluation
            evaluation = get_gpt4_evaluation(prompt, base_response, instruct_response)
            
            # Store results
            results.append({
                "category": category,
                "prompt": prompt,
                "base_response": base_response,
                "instruct_response": instruct_response,
                "gpt4_evaluation": evaluation,
                "timestamp": datetime.now().isoformat()
            })
    
    return results

def save_results(results):
    # Save as JSON for raw data
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    with open(f"evaluation_results_{timestamp}.json", "w") as f:
        json.dump(results, f, indent=2)
    
    # Create summary statistics
    summary = calculate_evaluation_summary(results)
    
    with open(f"evaluation_summary_{timestamp}.json", "w") as f:
        json.dump(summary, f, indent=2)
    
    print(f"\nEvaluation complete! Processed {len(results)} prompts")
    print(f"Results saved as evaluation_results_{timestamp}.json")
    print(f"Summary saved as evaluation_summary_{timestamp}.json")

def calculate_evaluation_summary(results):
    """Calculate average scores and statistics from the evaluations."""
    base_scores = {
        "relevance": [], "accuracy": [], "coherence": [], 
        "conciseness": [], "task_completion": []
    }
    instruct_scores = {
        "relevance": [], "accuracy": [], "coherence": [], 
        "conciseness": [], "task_completion": []
    }
    
    for result in results:
        if "gpt4_evaluation" in result and "error" not in result["gpt4_evaluation"]:
            eval_data = result["gpt4_evaluation"]
            for metric in base_scores.keys():
                if metric in eval_data["base_model"]:
                    base_scores[metric].append(eval_data["base_model"][metric])
                if metric in eval_data["instruct_model"]:
                    instruct_scores[metric].append(eval_data["instruct_model"][metric])
    
    # Calculate averages
    summary = {
        "base_model_averages": {
            metric: sum(scores)/len(scores) if scores else 0 
            for metric, scores in base_scores.items()
        },
        "instruct_model_averages": {
            metric: sum(scores)/len(scores) if scores else 0 
            for metric, scores in instruct_scores.items()
        },
        "sample_size": len(results),
        "timestamp": datetime.now().isoformat()
    }
    
    return summary

if __name__ == "__main__":
    print("Starting batch evaluation with GPT-4 judging...")
    results = run_batch_evaluation()
    save_results(results)