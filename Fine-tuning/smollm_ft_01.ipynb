{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Max\\Documents\\Software\\5930\\finenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "CUDA device: NVIDIA GeForce RTX 4070 Ti SUPER\n",
      "CUDA version: 11.8\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
    "from datasets import load_dataset, Dataset\n",
    "import json\n",
    "\n",
    "# CUDA settings\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "torch.backends.cudnn.enabled = True\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# Check CUDA availability and set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    # Set default CUDA device\n",
    "    torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing and shit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer pad token: <|endoftext|>\n",
      "Tokenizer pad token ID: 0\n",
      "\n",
      "Downloading and loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model Loading:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load model directly to device...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Max\\Documents\\Software\\5930\\finenv\\lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Max\\.cache\\huggingface\\hub\\models--HuggingFaceTB--SmolLM2-135M. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Model Loading: 100%|██████████| 1/1 [01:21<00:00, 81.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model loaded successfully\n",
      "\n",
      "Model device check:\n",
      "Model is on CUDA: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "model_name = \"HuggingFaceTB/SmolLM2-135M\"\n",
    "\n",
    "# Load tokenizer and set padding token\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "print(f\"Tokenizer pad token: {tokenizer.pad_token}\")\n",
    "print(f\"Tokenizer pad token ID: {tokenizer.pad_token_id}\")\n",
    "\n",
    "# Load model with progress bar\n",
    "print(\"\\nDownloading and loading model...\")\n",
    "with tqdm(total=1, desc=\"Model Loading\", position=0, leave=True) as pbar:\n",
    "    try:\n",
    "        print(\"Attempting to load model directly to device...\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            device_map=\"auto\" if torch.cuda.is_available() else None,\n",
    "            torch_dtype=torch.float32\n",
    "        )\n",
    "        print(\"\\nModel loaded successfully\")\n",
    "        pbar.update(1)\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError loading model directly to device: {e}\")\n",
    "        print(\"Attempting alternate loading method...\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "        if torch.cuda.is_available():\n",
    "            print(\"Moving model to CUDA...\")\n",
    "            model = model.to('cuda')\n",
    "        print(\"Model loading complete\")\n",
    "        pbar.update(1)\n",
    "\n",
    "print(f\"\\nModel device check:\")\n",
    "print(f\"Model is on CUDA: {next(model.parameters()).is_cuda}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing tokenization function:\n",
      "\n",
      "input_ids:\n",
      "Shape: torch.Size([2, 512])\n",
      "Type: torch.int64\n",
      "Example decoded: Here is a sample text.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
      "\n",
      "attention_mask:\n",
      "Shape: torch.Size([2, 512])\n",
      "Type: torch.int64\n",
      "\n",
      "labels:\n",
      "Shape: torch.Size([2, 512])\n",
      "Type: torch.int64\n"
     ]
    }
   ],
   "source": [
    "# Define max length for the sequences\n",
    "MAX_LENGTH = 512\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    \"\"\"\n",
    "    Tokenize texts and prepare them for training with SmolLM2.\n",
    "    Properly handles attention masks and padding.\n",
    "    \n",
    "    Args:\n",
    "        examples: Dataset examples containing 'text' field\n",
    "    Returns:\n",
    "        dict: Tokenized examples with input_ids, attention_mask, and labels\n",
    "    \"\"\"\n",
    "    # Tokenize with explicit attention mask and padding\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=MAX_LENGTH,\n",
    "        return_tensors=None,  # Return lists instead of tensors\n",
    "        return_attention_mask=True  # Explicitly request attention mask\n",
    "    )\n",
    "    \n",
    "    # For causal language modeling, labels are the input_ids\n",
    "    labels = tokenized[\"input_ids\"].copy()\n",
    "    \n",
    "    # Convert to tensors\n",
    "    tokenized = {\n",
    "        \"input_ids\": torch.tensor(tokenized[\"input_ids\"], dtype=torch.long),\n",
    "        \"attention_mask\": torch.tensor(tokenized[\"attention_mask\"], dtype=torch.long),\n",
    "        \"labels\": torch.tensor(labels, dtype=torch.long)\n",
    "    }\n",
    "\n",
    "    return tokenized\n",
    "\n",
    "# Test the tokenization function\n",
    "test_data = Dataset.from_dict({\n",
    "    \"text\": [\n",
    "        \"Here is a sample text.\",\n",
    "        \"Here is another, longer piece of text that might need padding.\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"Testing tokenization function:\")\n",
    "tokenized_output = tokenize_function(test_data)\n",
    "for key, value in tokenized_output.items():\n",
    "    print(f\"\\n{key}:\")\n",
    "    print(f\"Shape: {value.shape}\")\n",
    "    print(f\"Type: {value.dtype}\")\n",
    "    if key == \"input_ids\":\n",
    "        print(\"Example decoded:\", tokenizer.decode(value[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "accept dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset_for_training(\n",
    "    data_source,\n",
    "    text_field=\"text\",\n",
    "    train_size=0.9,\n",
    "    max_samples=None,\n",
    "    seed=42\n",
    "):\n",
    "    \"\"\"\n",
    "    Prepare any dataset for training SmolLM2.\n",
    "    \n",
    "    Args:\n",
    "        data_source: Can be:\n",
    "            - path to .txt file (one sample per line)\n",
    "            - path to .json file\n",
    "            - list of texts\n",
    "            - HuggingFace dataset\n",
    "        text_field: Name of the text field if using structured data\n",
    "        train_size: Proportion to use for training (0 to 1)\n",
    "        max_samples: Optional limit on dataset size\n",
    "        seed: Random seed for reproducibility\n",
    "    \"\"\"\n",
    "    print(\"Loading dataset...\")\n",
    "    \n",
    "    # Handle different input types\n",
    "    if isinstance(data_source, str):\n",
    "        # File path provided\n",
    "        if data_source.endswith('.json'):\n",
    "            with open(data_source, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            dataset = Dataset.from_list(data)\n",
    "        elif data_source.endswith('.txt'):\n",
    "            with open(data_source, 'r', encoding='utf-8') as f:\n",
    "                texts = [line.strip() for line in f if line.strip()]\n",
    "            dataset = Dataset.from_dict({\"text\": texts})\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported file format\")\n",
    "    elif isinstance(data_source, list):\n",
    "        # List of texts\n",
    "        dataset = Dataset.from_dict({\"text\": data_source})\n",
    "    else:\n",
    "        # Assume it's already a dataset\n",
    "        dataset = data_source\n",
    "    \n",
    "    # Limit dataset size if specified\n",
    "    if max_samples and len(dataset) > max_samples:\n",
    "        dataset = dataset.select(range(max_samples))\n",
    "    \n",
    "    print(f\"Total samples: {len(dataset)}\")\n",
    "    \n",
    "    # Split into train/validation\n",
    "    train_dataset, eval_dataset = dataset.train_test_split(\n",
    "        train_size=train_size,\n",
    "        seed=seed\n",
    "    )\n",
    "    \n",
    "    print(\"Tokenizing datasets...\")\n",
    "    \n",
    "    # Tokenize both splits\n",
    "    tokenized_train = train_dataset.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        remove_columns=train_dataset.column_names\n",
    "    )\n",
    "    tokenized_eval = eval_dataset.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        remove_columns=eval_dataset.column_names\n",
    "    )\n",
    "    \n",
    "    # Set format for PyTorch\n",
    "    tokenized_train.set_format(\"torch\")\n",
    "    tokenized_eval.set_format(\"torch\")\n",
    "    \n",
    "    print(f\"\\nFinal dataset sizes:\")\n",
    "    print(f\"Training: {len(tokenized_train)}\")\n",
    "    print(f\"Evaluation: {len(tokenized_eval)}\")\n",
    "    \n",
    "    return tokenized_train, tokenized_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fine-tuning setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Max\\Documents\\Software\\5930\\finenv\\lib\\site-packages\\transformers\\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from typing import Dict, List\n",
    "\n",
    "@dataclass\n",
    "class SmolDataCollator:\n",
    "    \"\"\"\n",
    "    Custom data collator for SmolLM2 training\n",
    "    \"\"\"\n",
    "    def __call__(self, examples: List[Dict[str, torch.Tensor]]) -> Dict[str, torch.Tensor]:\n",
    "        # Stack all the input tensors together\n",
    "        input_ids = torch.stack([example[\"input_ids\"] for example in examples])\n",
    "        attention_mask = torch.stack([example[\"attention_mask\"] for example in examples])\n",
    "        labels = torch.stack([example[\"labels\"] for example in examples])\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": labels\n",
    "        }\n",
    "\n",
    "# Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=100,\n",
    "    logging_steps=100,\n",
    "    save_total_limit=2,\n",
    "    logging_dir=\"./logs\",\n",
    "    max_grad_norm=0.5,\n",
    "    gradient_accumulation_steps=8,\n",
    "    fp16=False,  # Set to True if you have GPU with CUDA capability\n",
    "    dataloader_pin_memory=False if not torch.cuda.is_available() else True,\n",
    "    remove_unused_columns=False\n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "data_collator = SmolDataCollator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2/2 [00:00<00:00, 138.25 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test batch shapes:\n",
      "input_ids: torch.Size([2, 512]), dtype: torch.int64, range: [0, 6330]\n",
      "attention_mask: torch.Size([2, 512]), dtype: torch.int64, range: [0, 1]\n",
      "labels: torch.Size([2, 512]), dtype: torch.int64, range: [0, 6330]\n",
      "\n",
      "Setup validation complete. Ready for training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# First prepare test datasets\n",
    "dummy_dataset = Dataset.from_dict({\n",
    "    \"text\": [\"This is a test sentence.\", \"Another test sentence.\"]\n",
    "})\n",
    "tokenized_test = dummy_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=dummy_dataset.column_names\n",
    ")\n",
    "tokenized_test.set_format(\"torch\")\n",
    "\n",
    "# Split into train/eval\n",
    "train_test, eval_test = tokenized_test.train_test_split(test_size=0.5, seed=42)\n",
    "\n",
    "# Initialize trainer with both train and eval datasets\n",
    "test_trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_test,\n",
    "    eval_dataset=eval_test,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Create and inspect a test batch\n",
    "test_batch = data_collator([tokenized_test[i] for i in range(len(tokenized_test))])\n",
    "print(\"\\nTest batch shapes:\")\n",
    "for k, v in test_batch.items():\n",
    "    print(f\"{k}: {v.shape}, dtype: {v.dtype}, range: [{v.min()}, {v.max()}]\")\n",
    "\n",
    "print(\"\\nSetup validation complete. Ready for training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear CUDA cache before training if using GPU\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "def scale_dataset(dataset, max_samples=1000):\n",
    "    \"\"\"Scale down a dataset to a maximum number of samples\"\"\"\n",
    "    if len(dataset) > max_samples:\n",
    "        scaled_indices = list(range(max_samples))\n",
    "        return dataset.select(scaled_indices)\n",
    "    return dataset\n",
    "\n",
    "def train_model(train_dataset, eval_dataset, max_train_samples=None, max_eval_samples=None):\n",
    "    # Scale datasets if specified\n",
    "    print(f\"Original sizes - Train: {len(train_dataset)}, Eval: {len(eval_dataset)}\")\n",
    "    \n",
    "    if max_train_samples:\n",
    "        train_dataset = scale_dataset(train_dataset, max_train_samples)\n",
    "        # Scale eval set proportionally (usually ~10% of train size)\n",
    "        if not max_eval_samples:\n",
    "            max_eval_samples = max(50, int(max_train_samples * 0.1))\n",
    "    \n",
    "    if max_eval_samples:\n",
    "        eval_dataset = scale_dataset(eval_dataset, max_eval_samples)\n",
    "    \n",
    "    print(f\"Scaled sizes - Train: {len(train_dataset)}, Eval: {len(eval_dataset)}\")\n",
    "\n",
    "    # Initialize trainer with scaled data\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "    \n",
    "    # Start training\n",
    "    try:\n",
    "        print(\"Starting training...\")\n",
    "        trainer_output = trainer.train()\n",
    "        \n",
    "        # Save the model\n",
    "        print(\"Saving model...\")\n",
    "        trainer.save_model(\"./smollm2_finetuned\")\n",
    "        tokenizer.save_pretrained(\"./smollm2_finetuned\")\n",
    "        \n",
    "        return trainer_output\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Training error: {e}\")\n",
    "        if torch.cuda.is_available():\n",
    "            print(\"\\nCUDA Memory Summary:\")\n",
    "            print(torch.cuda.memory_summary())\n",
    "        raise\n",
    "\n",
    "# Usage example:\n",
    "# train_output = train_model(\n",
    "#     train_dataset, \n",
    "#     eval_dataset, \n",
    "#     max_train_samples=40000,  # Adjust these numbers as needed\n",
    "#     max_eval_samples=4000\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
